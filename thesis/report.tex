\documentclass[11pt,a4paper]{report}

\usepackage[titletoc]{appendix}
\usepackage[intoc]{nomencl}
\usepackage{setspace}
\usepackage{pdfpages}
\usepackage{tocbibind}

\newcommand{\quot}[1]{``#1''}
\boldmath

\singlespacing
\makenomenclature

\begin{document}
	\includepdf{cover}
	
	\begin{abstract}
		This is the abstract.
	\end{abstract}
	
	\tableofcontents

	\printnomenclature
	\listoffigures
	\listoftables
	
	\chapter{Introduction}
		\nomenclature{y}{A very cool variable.}
		\section{The problem}
		Artificial neural networks in all their various incarnations have been successfully used to solve a very wide range of machine learning problems, thanks to their good representational capabilities \cite{sharma2010constructive}.
		
		Theoretically, a simple feed-forward neural network with a single hidden layer and a sufficient number of neurons in that layer is an universal approximator \cite{hornik1989multilayer}. In practice however, too small networks may be unable to adequately learn the problem, while overly large networks tend to overfit the training data \cite{parekh2000constructive}.
		
		The problem of determining the optimal neural network topology is thus very important. However, there are currently no efficient methods to choose \emph{a priori} the best network architecture for a given problem \cite{parekh2000constructive}. In real-world applications, \emph{trial-and-error} and decisions based on previous knowledge about the problem are often the preferred approach. The chosen architecture has thus no guarantees to be the optimal one for the task \cite{sharma2010constructive}.
		
		To overcome this problem, solutions that involve learning both the weights of the synapses \emph{and} the network topology have been suggested \cite{parekh2000constructive}.
		
		One of the proposed answers is the class of algorithms of the so-called \emph{constructive neural networks}. The main idea behind it is starting from a minimal architecture and then adding hidden layers, nodes and connections during training \cite{sharma2010constructive}.
		
		Such algorithms provides the flexibility to explore the space of neural network topologies in a controlled, \emph{data-driven} fashion. Furthermore, because small solutions are found first, this method has the potential to discover near-minimal networks that approximately match the complexity of the learning task \cite{parekh2000constructive}.
		
		For all these reasons, applying a constructive approach to existing learning algorithms constitutes an interesting problem.
		
		\newpage
		
		In the present project I focused on the development of a constructive version of the DIM\footnote{Divisive Input Modulation.} learning algorithm, as described in \cite{spratling2009unsupervised}, and its derivation \cite{spratling2012unsupervised} used in the \emph{non-linear} PC/BC model\footnote{Predictive Coding as Biased Competition} \cite{spratling2008predictive}. The DIM algorithm has been developed to train a variation of \emph{negative feedback networks} and has proven to be very successful in the unsupervised learning of image components, yielding state-of-the-art performances in some standard benchmarks.
		
		One one hand, the main feature of negative feedback networks is the competition between nodes, which enables them to be selective for different input stimuli by making the individual synaptic weights more distinct. This is achieved by having inhibiting (\emph{divisive}) feedback connections from the output neurons back to their input nodes \cite{spratling2009unsupervised}. 
		
		\begin{itemize}
			\item Show some images of the network structure.
		\end{itemize}
		
		On the other hand, the PC model proposes a hierarchical architecture in which alternating populations of \emph{error-detecting} and \emph{predicting} nodes interact with each other to carry out the perception process \cite{spratling2014predictive}. In particular, the higher levels of the network are not limited to passively receiving the input from the preceding nodes, but instead they actively predict the input they expect to receive. Feedback connections convey the predictions, while feedforward connection transmit the residual error between those predictions and actual input \cite{spratling2008predictive}.

		A connection can be drawn between these two models. If we consider negative feedback networks from an equivalent perspective, namely a generative one, it can be shown that the higher level of the network produces a \emph{reconstruction} of the input via the feedback connections; while feedforward connections instead convey \emph{residual error} between the top-down prediction and the bottom-up input \cite{spratling2009unsupervised}, much like in the PC model.
		Symmetrically, \emph{Predictive Coding} can be redefined as a form of \emph{Biased Competition} \cite{spratling2008predictive}.
		
		A PC/BC network trained using the DIM algorithm is referred to as PC/BC--DIM model. Most notably, in this model the training of the weights of feedforward and feedback connections can be performed simultaneously and independently, thus providing a biologically sound computational theory explaining how reciprocal connections are learnt in actual cortical areas \cite{callaway1998local}\cite{spratling2012unsupervised}.
		
		\section{Goals of the project}
		The main focus of the project has been developing a version of the PC/BC--DIM algorithm with the ability to progressively evolve a near-optimal network topology alongside the feedforward and feedback weights. In the process, a constructive version of the basic DIM algorithm has also been implemented.
		
		The project aim is to provide an enhanced version of the original algorithm that performs equally well but produces smaller networks that can be trained faster.
		
		
		
		\begin{itemize}
			\item Show what neurons are we interested in growing by looking at the predictive model.
			\item Maybe show some images of the network?
			\item Describe the idea of having thresholds for error because of what error represents.
			\item Briefly mention literature about constructive network techniques.
			\item What kind of results I am getting.
			\item Contributions
		\end{itemize}

		\section{Structure of the report}
		\begin{itemize}
			\item Structure of the report.
		\end{itemize}
	
	\chapter{Literature Survey}
		\section{Background}
		\section{Theories}
	
	\chapter{Main Result}
		\section{Theoretical Development}
		\section{Analysis and Design}
		\section{Implementation and Experimental Work}
		\section{Results}
		\section{Discussion}
		
	\chapter{Conclusion}
	
	\bibliographystyle{plain}
	\bibliography{references}
	\nocite{*}
	
	\begin{appendices}
		\chapter{Source Code}
	\end{appendices}
\end{document}
